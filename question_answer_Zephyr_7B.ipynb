{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJROgk8L3J8jZT6ilItB2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiwihead15/Test-models/blob/main/question_answer_Zephyr_7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install chainlit langchain"
      ],
      "metadata": {
        "id": "j7A0aySHOawS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyxFE9_7OCJ7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import chainlit as cl\n",
        "from langchain.llms import CTransformers\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_llm = \"zephyr-7b-alpha.Q8_0.gguf\"  # Path to the local Zephyr-7B model file\n",
        "\n",
        "config = {\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.9,\n",
        "    \"stream\": True,\n",
        "    \"threads\": int(os.cpu_count() / 2)\n",
        "}"
      ],
      "metadata": {
        "id": "GXLI7XhDOyYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Please refer to factual information and don't make up fictional data/information.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m8oL05n2O4O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cl.on_chat_start\n",
        "def main():\n",
        "    prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm_init, verbose=True)\n",
        "    cl.user_session.set(\"llm_chain\", llm_chain)"
      ],
      "metadata": {
        "id": "hhXG3A_XO8iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cl.on_message\n",
        "async def main(message: str):\n",
        "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
        "    res = await llm_chain.acall(message, callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
        "    await cl.Message(content=res[\"text\"]).send()"
      ],
      "metadata": {
        "id": "BFEkvUE8PDGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chainlit run <name-oython-script>.py"
      ],
      "metadata": {
        "id": "OyGJ1zI_PICR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}